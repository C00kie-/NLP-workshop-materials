{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Build a sentiment analysis pipeline with HuggingFace","metadata":{}},{"cell_type":"code","source":"#for colab\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:44.894602Z","iopub.execute_input":"2023-10-05T01:28:44.894932Z","iopub.status.idle":"2023-10-05T01:28:53.396503Z","shell.execute_reply.started":"2023-10-05T01:28:44.894905Z","shell.execute_reply":"2023-10-05T01:28:53.395341Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:53.399069Z","iopub.execute_input":"2023-10-05T01:28:53.399472Z","iopub.status.idle":"2023-10-05T01:28:53.403849Z","shell.execute_reply.started":"2023-10-05T01:28:53.399433Z","shell.execute_reply":"2023-10-05T01:28:53.402951Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"classifier = pipeline(\"sentiment-analysis\")","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:53.405209Z","iopub.execute_input":"2023-10-05T01:28:53.406151Z","iopub.status.idle":"2023-10-05T01:28:54.358669Z","shell.execute_reply.started":"2023-10-05T01:28:53.406120Z","shell.execute_reply":"2023-10-05T01:28:54.357728Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We start by creating a \"Sentiment Analysis\" **classifier** using the pipeline function provided by the Hugging Face Transformers library. This function allows us to easily use pre-trained models for various natural language processing (NLP) tasks, like sentiment analysis.","metadata":{}},{"cell_type":"code","source":"results = classifier(\"This is cool\")\nresults","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:54.360980Z","iopub.execute_input":"2023-10-05T01:28:54.361836Z","iopub.status.idle":"2023-10-05T01:28:54.408540Z","shell.execute_reply.started":"2023-10-05T01:28:54.361802Z","shell.execute_reply":"2023-10-05T01:28:54.407588Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[{'label': 'POSITIVE', 'score': 0.9998584985733032}]"},"metadata":{}}]},{"cell_type":"markdown","source":"The model takes this text as input and predicts the sentiment associated with it. ","metadata":{}},{"cell_type":"markdown","source":"pipeline doc: https://huggingface.co/docs/transformers/main_classes/pipelines\npipeline tasks: ","metadata":{}},{"cell_type":"markdown","source":"### More then one sentence","metadata":{}},{"cell_type":"code","source":"# We give a list to the classifier now\nresults = classifier([\"NLP is nice\", \"It's a lot of work\"])\nresults","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:54.410001Z","iopub.execute_input":"2023-10-05T01:28:54.410534Z","iopub.status.idle":"2023-10-05T01:28:54.508009Z","shell.execute_reply.started":"2023-10-05T01:28:54.410502Z","shell.execute_reply":"2023-10-05T01:28:54.507107Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[{'label': 'POSITIVE', 'score': 0.9997960925102234},\n {'label': 'POSITIVE', 'score': 0.9995623230934143}]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Exercise:\n\nAdd different text inputs with varying sentiments, run it, check the model's sentiment predictions, and explore how it assigns labels.","metadata":{}},{"cell_type":"markdown","source":"## Now select a specific model into your pipeline","metadata":{}},{"cell_type":"code","source":"model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:54.509514Z","iopub.execute_input":"2023-10-05T01:28:54.509823Z","iopub.status.idle":"2023-10-05T01:28:54.514246Z","shell.execute_reply.started":"2023-10-05T01:28:54.509792Z","shell.execute_reply":"2023-10-05T01:28:54.513239Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"The model_name variable holds the name of the pre-trained model. In this case, it's \"twitter-roberta-base-sentiment-latest\"","metadata":{}},{"cell_type":"markdown","source":"Let's have a look at the model card: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest","metadata":{}},{"cell_type":"code","source":"classifier = pipeline(\"sentiment-analysis\", model=model_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:54.515806Z","iopub.execute_input":"2023-10-05T01:28:54.516392Z","iopub.status.idle":"2023-10-05T01:28:55.355163Z","shell.execute_reply.started":"2023-10-05T01:28:54.516361Z","shell.execute_reply":"2023-10-05T01:28:55.354221Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"markdown","source":"- Tokenization is the process of breaking down text into smaller **units** called **tokens**.\n\n- Tokens are the basic building blocks used by Transformers models to understand and process text.\n\n- Tokens can represent **words, subwords, or even individual characters**, depending on the model's vocabulary.","metadata":{}},{"cell_type":"markdown","source":"![Pipeline](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)","metadata":{}},{"cell_type":"markdown","source":"Source image: https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:55.356390Z","iopub.execute_input":"2023-10-05T01:28:55.356719Z","iopub.status.idle":"2023-10-05T01:28:55.361422Z","shell.execute_reply.started":"2023-10-05T01:28:55.356689Z","shell.execute_reply":"2023-10-05T01:28:55.360527Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"\"AutoModelForSequenceClassification\" adapts to various model architectures automatically.","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:55.362893Z","iopub.execute_input":"2023-10-05T01:28:55.363556Z","iopub.status.idle":"2023-10-05T01:28:56.082049Z","shell.execute_reply.started":"2023-10-05T01:28:55.363525Z","shell.execute_reply":"2023-10-05T01:28:56.081052Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"When using from_pretrained, we are loading a pre-trained model and tokenizer specified by the model_name.","metadata":{}},{"cell_type":"code","source":"classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.085563Z","iopub.execute_input":"2023-10-05T01:28:56.085867Z","iopub.status.idle":"2023-10-05T01:28:56.105959Z","shell.execute_reply.started":"2023-10-05T01:28:56.085844Z","shell.execute_reply":"2023-10-05T01:28:56.105136Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"We create our sentiment analysis classifier.","metadata":{}},{"cell_type":"markdown","source":"## Tokens to inputs IDs","metadata":{}},{"cell_type":"code","source":"tokens = tokenizer.tokenize(\"Another cool sentence to demonstrate something.\")\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\ninput_ids = tokenizer(\"Another cool sentence to demonstrate something.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.107285Z","iopub.execute_input":"2023-10-05T01:28:56.107623Z","iopub.status.idle":"2023-10-05T01:28:56.112778Z","shell.execute_reply.started":"2023-10-05T01:28:56.107593Z","shell.execute_reply":"2023-10-05T01:28:56.111516Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print(f' Tokens:{tokens}')\nprint(f' Token IDs: {token_ids}')\nprint(f' input_ids:{input_ids}')","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.114134Z","iopub.execute_input":"2023-10-05T01:28:56.114684Z","iopub.status.idle":"2023-10-05T01:28:56.124564Z","shell.execute_reply.started":"2023-10-05T01:28:56.114652Z","shell.execute_reply":"2023-10-05T01:28:56.123480Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":" Tokens:['another', 'cool', 'sentence', 'to', 'demonstrate', 'something', '.']\n Token IDs: [2178, 4658, 6251, 2000, 10580, 2242, 1012]\n input_ids:{'input_ids': [101, 2178, 4658, 6251, 2000, 10580, 2242, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exercise: \nTest different tokenizers, select models from the hub.","metadata":{}},{"cell_type":"markdown","source":"Some:\n\nhttps://huggingface.co/SamLowe/roberta-base-go_emotions\n\nhttps://huggingface.co/bert-base-uncased\n\nSome more... \n","metadata":{}},{"cell_type":"code","source":"#uncomment this to answer the exercise\n#tokenizer = AutoTokenizer.from_pretrained(\"[model]\")\n#tokens = tokenizer.tokenize(\"Woaou another sentence!\")\n#token_ids = tokenizer.convert_tokens_to_ids(tokens)\n#input_ids = tokenizer(\"Another cool sentence to demonstrate something.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.126010Z","iopub.execute_input":"2023-10-05T01:28:56.126637Z","iopub.status.idle":"2023-10-05T01:28:56.134903Z","shell.execute_reply.started":"2023-10-05T01:28:56.126601Z","shell.execute_reply":"2023-10-05T01:28:56.133912Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print(f' Tokens:{tokens}')\nprint(f' Token IDs: {token_ids}')\nprint(f' input_ids:{input_ids}')","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.136448Z","iopub.execute_input":"2023-10-05T01:28:56.136788Z","iopub.status.idle":"2023-10-05T01:28:56.146210Z","shell.execute_reply.started":"2023-10-05T01:28:56.136757Z","shell.execute_reply":"2023-10-05T01:28:56.145210Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":" Tokens:['another', 'cool', 'sentence', 'to', 'demonstrate', 'something', '.']\n Token IDs: [2178, 4658, 6251, 2000, 10580, 2242, 1012]\n input_ids:{'input_ids': [101, 2178, 4658, 6251, 2000, 10580, 2242, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Batching","metadata":{}},{"cell_type":"code","source":"sentences = [\"Another cool sentence to demonstrate something.\",\n           \"All I need is two sentences.\"]\nbatch = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\") #pt for pyTorch","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.147635Z","iopub.execute_input":"2023-10-05T01:28:56.148036Z","iopub.status.idle":"2023-10-05T01:28:56.159893Z","shell.execute_reply.started":"2023-10-05T01:28:56.148006Z","shell.execute_reply":"2023-10-05T01:28:56.158926Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Note:\nAll our sample will have the same length (necessity for the model) - tensors must have the same shape.\n```\npadding=True and truncation=True\n```","metadata":{}},{"cell_type":"code","source":"pprint(batch)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.161712Z","iopub.execute_input":"2023-10-05T01:28:56.162783Z","iopub.status.idle":"2023-10-05T01:28:56.172125Z","shell.execute_reply.started":"2023-10-05T01:28:56.162754Z","shell.execute_reply":"2023-10-05T01:28:56.171081Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n 'input_ids': tensor([[  101,  2178,  4658,  6251,  2000, 10580,  2242,  1012,   102],\n        [  101,  2035,  1045,  2342,  2003,  2048, 11746,  1012,   102]])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Returns a dictionary with keys 'input_ids' and 'attention_mask', with two tensors the 'input ids' tensor and the 'attention_mask' tensor.\ninput_ids are unique ids.","metadata":{}},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"code","source":"# Prevent gradient computation (no need to compute gradients during inference)\n\nwith torch.no_grad():\n    outputs = model(**batch) \n    print(outputs)\n    print('')\n    predictions = torch.softmax(outputs.logits, dim=1)      # Apply softmax to convert model logits to probabilities\n    pprint(predictions)\n    print('')\n    labels = torch.argmax(predictions, dim=1)              # Find the index of the class with the highest probability for each example\n    pprint(labels)\n    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n    pprint(labels)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.174286Z","iopub.execute_input":"2023-10-05T01:28:56.175042Z","iopub.status.idle":"2023-10-05T01:28:56.235479Z","shell.execute_reply.started":"2023-10-05T01:28:56.175013Z","shell.execute_reply":"2023-10-05T01:28:56.234562Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"SequenceClassifierOutput(loss=None, logits=tensor([[-3.9261,  4.2183],\n        [ 2.8756, -2.4102]]), hidden_states=None, attentions=None)\n\ntensor([[2.9026e-04, 9.9971e-01],\n        [9.9496e-01, 5.0377e-03]])\n\ntensor([1, 0])\n['POSITIVE', 'NEGATIVE']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the number of decimal places to round to\ndecimal_places = 2\n# Round the probabilities\nrounded_probabilities = torch.round(predictions * 10**decimal_places) / (10**decimal_places)\n# Print the rounded probabilities\nprint('')\npprint(rounded_probabilities)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.236846Z","iopub.execute_input":"2023-10-05T01:28:56.237212Z","iopub.status.idle":"2023-10-05T01:28:56.244170Z","shell.execute_reply.started":"2023-10-05T01:28:56.237180Z","shell.execute_reply":"2023-10-05T01:28:56.243187Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"\ntensor([[0.0000, 1.0000],\n        [0.9900, 0.0100]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Saving","metadata":{}},{"cell_type":"code","source":"save_directory = \"your_dir\"\ntokenizer.save_pretrained(save_directory)\nmodel. save_pretrained(save_directory)\n\ntokenizer = AutoTokenizer.from_pretrained(save_directory)\nmodel = AutoModelForSequenceClassification.from_pretrained(save_directory)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T01:28:56.245722Z","iopub.execute_input":"2023-10-05T01:28:56.246373Z","iopub.status.idle":"2023-10-05T01:28:57.751592Z","shell.execute_reply.started":"2023-10-05T01:28:56.246340Z","shell.execute_reply":"2023-10-05T01:28:57.750613Z"},"trusted":true},"execution_count":51,"outputs":[]}]}
